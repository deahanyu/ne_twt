---
title: "twt_ne_food"
author: "Deahan Yu"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown
---
```{r echo=FALSE, warning=FALSE, comment=FALSE, results=FALSE}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(data.table)
library(ggplot2)
library(gridExtra)
library(glmnet)
#library(knockoff)
#library(gam)
library(pls)
library(randomForest)
library(caret)
```

```{r echo=FALSE, warning=FALSE, comment=FALSE,results=FALSE}
dat<-matrix(NA,3,2)
dat[1,]<-c(1,1)
dat[2,]<-c(2,2)
dat[3,1]<-3
# dat
lm<-lm(dat[,1]~dat[,2])
# lm$coef
# lm$model

dat[3,2]<-10
lm<-lm(dat[,1]~dat[,2])
# lm$coef
# lm$model
```
#Initial Step
```{r echo=FALSE, warning=FALSE} 
dt <- data.table(read.csv(Sys.glob('output/*.csv'),header = TRUE, sep = ','))
dt<-dt[,-(which(colnames(dt)=="num_healthy1"):which(colnames(dt)=="net_score"))]
dt<-dt[,-(which(colnames(dt)=="num_unique_users"):which(colnames(dt)=="net_healthy_unhealthy_related_tweets"))]

for (i in colnames(dt)) {
  if (any(i==c("GEOID10","tract","CountyCode","CountyName","countyname","highbroadbandadoptionrate"))){
    dt[,i] <- as.factor(as.character(dt[[i]]))
  } else if(length(grep("[.]",i))>0 || length(grep("Mortality",i))>0 || length(grep("Sentiment",i))>0 || length(grep("total_twts",i))>0||length(grep("recrecen",i))>0||length(grep("Grocery_Store_AvgDistance",i))>0 || length(grep("Voter_Turnout",i))>0||length(grep("Parkland",i))>0) {
    #deleting variables that has .1 or .2 at the end.  (duplicated ones)
     dt[[i]]<-NULL 
  }
  else{
    dt[,i] <- as.numeric(as.character(dt[[i]]))
  }
}

#dropping variables that are >=400
maxNum <- ncol(dt)
for (j in maxNum:1){
  if (length(which(is.na(dt[[j]])==TRUE))>=1){
    if (length(which(is.na(dt[[j]])==TRUE))>=400){
      #print (paste(colnames(dt)[j],length(which(is.na(dt[[j]])==TRUE)),sep=" : "))
      dt[[j]] <- NULL
    }
  }
}
#IV's end after "GEOID"
IVnumbers<-which(colnames(dt)=="GEOID10")-1
#colnames(dt) this is after dropping columns more than 400 na's

```
#Exploratory
##Our normalization methods
```
Normalization A - Num food words
  Score healthy     /   Num food words
  Score unhealthy   /   Num food words
  Net score         /   Num food words

Normalization B - Food/Alcohol related tweets
  Score healthy     /   Food related tweets
  Score unhealthy   /   Food related tweets
  Net score         /   Food related tweets
  Num alcohol words /   Alcohol related tweets
  
Tract-level 
  tweet-level  -->  tract-level  -->  normalization 

User-level
  tweet-level  -->  user-level   -->  normalization  --> user-count average of tract-level
```

```
Different norms  A vs. B                             Same norms      AA and BB    
Same level       TT and UU                       Different level     Tact vs. User
```
```{r echo=FALSE, warning=FALSE}
makeGgp<-function(ddff,xx,xxlab,yy,yylab,mmin,mmax){
  ggp<-ggplot(ddff,aes(x=xx, y=yy))#,fill=countyname))
  ggp<-ggp+geom_point()#(aes(colour=countyname))
  ggp<-ggp+coord_cartesian(xlim=c(mmin,mmax),ylim=c(mmin,mmax))
  ggp<-ggp+geom_abline(slope=1, intercept=0)
  ggp<-ggp+labs(x=xxlab,y=yylab)
  return(ggp)
}
prettyScatterplot<-function(ddff,x1,x1lab,y1,y1lab,x2,x2lab,y2,y2lab,ttitle){
  fMin <- min(summary(x1),summary(x2),summary(y1),summary(y2))
  fMax <- max(summary(x1),summary(x2),summary(y1),summary(y2))
  ggp <- makeGgp(ddff,x1,x1lab,y1,y1lab,fMin,fMax)
  ggp1<- makeGgp(ddff,y2,y2lab,y1,y1lab,fMin,fMax)
  ggp2<- makeGgp(ddff,x2,x2lab,y2,y2lab,fMin,fMax)
  ggp3<- makeGgp(ddff,x2,x2lab,x1,x1lab,fMin,fMax)
  return(grid.arrange(ggp,ggp1,ggp2,ggp3, nrow=2,ncol=2,top = ttitle))
}

prettyScatterplot(dt,dt$NormB_tract_score_healthy,"NormB_tract_score_healthy",dt$NormA_tract_score_healthy,"NormA_tract_score_healthy",dt$NormB_user_score_healthy,"NormB_user_score_healthy",dt$NormA_user_score_healthy,"NormA_user_score_healthy","score_healthy")

prettyScatterplot(dt,dt$NormB_tract_score_unhealthy,"NormB_tract_score_unhealthy",dt$NormA_tract_score_unhealthy,"NormA_tract_score_unhealthy",dt$NormB_user_score_unhealthy,"NormB_user_score_unhealthy",dt$NormA_user_score_unhealthy,"NormA_user_score_unhealthy","score_unhealthy")

prettyScatterplot(dt,dt$NormB_tract_net_score,"NormB_tract_net_score",dt$NormA_tract_net_score,"NormA_tract_net_score",dt$NormB_user_net_score,"NormB_user_net_score",dt$NormA_user_net_score,"NormA_user_net_score","net_score")

fMin<-min(summary(dt$NormB_tract_num_alcohol_words),summary(dt$NormB_user_num_alcohol_words))
fMax<-max(summary(dt$NormB_tract_num_alcohol_words),summary(dt$NormB_user_num_alcohol_words))
ggp<-ggplot(dt,aes(y=dt$NormB_tract_num_alcohol_words, x=dt$NormB_user_num_alcohol_words))+geom_point()+coord_cartesian(xlim=c(fMin,fMax),ylim=c(fMin,fMax))+geom_abline(slope=1, intercept=0)+labs(y="NormB_tract_num_alcohol_words",x="NormB_user_num_alcohol_words")
pseudodt<- data.frame()
ggp1<-ggplot(pseudodt) + geom_point() + xlim(0, 10) + ylim(0, 10)
grid.arrange(ggp1, ggp, ncol=2,top="num_alcohol_words")
```

#Back to data analysis
##First step
```
Our data set contains 1593 census tracts. (1593 rows)

80% of census tracts with most tweets. 

Now 
```
```{r echo=FALSE, warning=FALSE}
print (paste0("1593 BY 26 --> ",paste(nrow(dt),ncol(dt),sep=" BY ")))
```

##Independent variables (IV)
```{r echo=FALSE, warning=FALSE}
#data prepartion for analyses
LOG10_twts_14_16<-log10(dt$num_tweets)
xtable<-cbind(dt[,1:IVnumbers],LOG10_twts_14_16)

#--For Food
#removing "LOG10_PCT_21_to_29_09_13", "LOG10_Liquor_Density_2015","LOG10_Religious_Density_2015"
xtable_food<-xtable
xtable_food$LOG10_PCT_21_to_29_09_13<-NULL
xtable_food$LOG10_Liquor_Density_2015<-NULL
xtable_food$LOG10_Religious_Density_2015<-NULL

#--For alcohol
#removing "LOG10_PCT_18_to_29_09_13","LOG10_fast_food_Density_2015"
xtable_alc<-xtable
xtable_alc$LOG10_PCT_18_to_29_09_13<-NULL
xtable_alc$LOG10_fast_food_Density_2015<-NULL
```
###IV's for Food 
```{r echo=FALSE}
print(xtable_food)
```
###IV's for alcohol
```{r echo=FALSE}
print(xtable_alc)
```

##Dependent variables (DV)
```{r echo=FALSE,warning=FALSE}
ys<-colnames(dt)[which(colnames(dt)=="NormA_tract_score_healthy"):ncol(dt)]

ytable<-dt[,ys,with=FALSE]
```
```
Normalization A - Num food words

  NormA_tract_score_healthy
  NormA_tract_score_unhealthy
  NormA_tract_net_score
  
  NormA_user_score_unhealthy
  NormA_user_net_score   
  NormA_user_score_healthy  

Normalization B - Food/Alcohol related tweets

  NormB_tract_score_healthy
  NormB_tract_score_unhealthy
  NormB_tract_net_score
  NormB_tract_num_alcohol_words
  
  NormB_user_score_healthy 
  NormB_user_score_unhealthy
  NormB_user_net_score   
  NormB_user_num_alcohol_words

```

#Exploratory Data Anlaysis - Independent Variables 
<!-- ##Summary Statistics  -->

<!-- ```{r echo=FALSE,warning=FALSE} -->
<!-- summary(xtable) -->
<!-- ``` -->

```{r echo=FALSE, warning=FALSE}
## - - -  linear regression functions are defined here. 
selectingXs <- function(ddtt,thisyy){
  thisXs<-c()
  for ( i in 1:ncol(ddtt)){
    thisdt <- cbind(thisyy,ddtt[[i]])
    eachlm<-lm(thisyy~V2,data=data.table(thisdt))
    # summary(eachlm)$r.squared
    f<-summary(eachlm)$fstatistic
    p<-pf(f[1],f[2],f[3],lower.tail=F)
    if (p < 0.15){
      thisXs<-c(thisXs,i)
     }
  }
  return(thisXs)
}

createfinal<-function(ddtt,xxx,yy){
  thisxtable<-ddtt[,xxx,with=FALSE]
  return(cbind(yy,thisxtable))
}

drawhist<-function(zzz,yyy){
  hist(rstandard(zzz),xlab="Regresion Standardized Residual",main=paste("Histogram\nDependent variable : ",yyy,sep=""))
}

drawpp<-function(zzz,yyy){
  # par(mfrow=c(2,1))
  # qqnorm(rstandard(lm_step),ylab="Standardized Residuals",xlab="Normal Scores") 
  # qqline(rstandard(lm_step))
  tmp1 <- rstandard(zzz)
  tmp2 <- pnorm( tmp1, 0, summary(zzz)$sigma )
  plot( ppoints(length(tmp1)),sort(tmp2), xlab="Theoretical Percentiles",ylab="Sample Percentiles",main=paste("Normal P-P Plot of Regression Standardized Residual\nDependent variable : ",yyy,sep=""))
  abline(0,1)
}

drawscatter<-function(zzz,yc,yyy){
  #only selected variables...(NA'sremoved by lm function automatically) (this is because we did not want to start with na.omit.....)
  onlyThis<-as.numeric(names(rstandard(zzz)))
  minz<-min(rstandard(zzz),na.omit(yc[onlyThis]))
  maxz<-max(rstandard(zzz),na.omit(yc[onlyThis]))
  plot(rstandard(zzz),yc[onlyThis],xlab="Regression Standardized Predicted Value",ylab=yyy,main=paste("Scatterplot\nDependent variable : ",yyy,sep=""),ylim=c(minz,maxz),xlim=c(minz,maxz))
  abline(0,1,h=0,v=0)
}

```

#Linear regression - NormA_user_net_score
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormA_user_net_score") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
```
##Simple Linear ression against each IV
```
ran simple linear regression against each IV 

selected any IV's - p-value is less than 0.10

```
### all available X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_food))
```
### not selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
x_vec<-selectingXs(xtable_food,y_vec)
print(colnames(xtable_food)[which(!(c(1:length(colnames(xtable_food))) %in% x_vec))])
```
### selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_food)[x_vec])
```

##Linear regression against selected IV's 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
selectedOnes<-createfinal(xtable_food,x_vec,y_vec)
slm<-lm(yy~.,data=selectedOnes)
summary(slm)
```
###Plots
```{r echo=FALSE, warning=FALSE}
drawhist(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawpp(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawscatter(slm,y_vec,y_name)
```

##Stepwise linear regression against selected IV's - dirction = forward 
```{r echo=FALSE, warning=FALSE, comment=FALSE,cache=FALSE, results=FALSE}
lm_step<-step(lm(yy~.,data=na.omit(selectedOnes)),direction="forward")
```
###Final Model Summary 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
summary(lm_step)
```
####Same result - plots are same above


#Linear regression - NormA_user_score_healthy
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormA_user_score_healthy") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
```
##Simple Linear ression against each IV
```
ran simple linear regression against each IV 

selected any IV's - p-value is less than 0.10

```
### all available X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_food))
```
### not selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
x_vec<-selectingXs(xtable_food,y_vec)
print(colnames(xtable_food)[which(!(c(1:length(colnames(xtable_food))) %in% x_vec))])
```
### selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_food)[x_vec])
```

##Linear regression against selected IV's 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
selectedOnes<-createfinal(xtable_food,x_vec,y_vec)
slm<-lm(yy~.,data=selectedOnes)
summary(slm)
```
###Plots
```{r echo=FALSE, warning=FALSE}
drawhist(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawpp(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawscatter(slm,y_vec,y_name)
```

##Stepwise linear regression against selected IV's - dirction = forward 
```{r echo=FALSE, warning=FALSE, comment=FALSE,cache=FALSE, results=FALSE}
lm_step<-step(lm(yy~.,data=na.omit(selectedOnes)),direction="forward")
```
###Final Model Summary 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
summary(lm_step)
```
####Same result - plots are same above


#Linear regression - NormA_user_score_unhealthy
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormA_user_score_unhealthy") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
```
##Simple Linear ression against each IV
```
ran simple linear regression against each IV 

selected any IV's - p-value is less than 0.10

```
### all available X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_food))
```
### not selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
x_vec<-selectingXs(xtable_food,y_vec)
print(colnames(xtable_food)[which(!(c(1:length(colnames(xtable_food))) %in% x_vec))])
```
### selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_food)[x_vec])
```

##Linear regression against selected IV's 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
selectedOnes<-createfinal(xtable_food,x_vec,y_vec)
slm<-lm(yy~.,data=selectedOnes)
summary(slm)
```
###Plots
```{r echo=FALSE, warning=FALSE}
drawhist(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawpp(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawscatter(slm,y_vec,y_name)
```

##Stepwise linear regression against selected IV's - dirction = forward 
```{r echo=FALSE, warning=FALSE, comment=FALSE,cache=FALSE, results=FALSE}
lm_step<-step(lm(yy~.,data=na.omit(selectedOnes)),direction="forward")
```
###Final Model Summary 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
summary(lm_step)
```
####Same result - plots are same above


#Linear regression - NormB_user_net_score
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormB_user_net_score") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
```
##Simple Linear ression against each IV
```
ran simple linear regression against each IV 

selected any IV's - p-value is less than 0.10

```
### all available X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_food))
```
### not selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
x_vec<-selectingXs(xtable_food,y_vec)
print(colnames(xtable_food)[which(!(c(1:length(colnames(xtable_food))) %in% x_vec))])
```
### selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_food)[x_vec])
```

##Linear regression against selected IV's 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
selectedOnes<-createfinal(xtable_food,x_vec,y_vec)
slm<-lm(yy~.,data=selectedOnes)
summary(slm)
```
###Plots
```{r echo=FALSE, warning=FALSE}
drawhist(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawpp(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawscatter(slm,y_vec,y_name)
```

##Stepwise linear regression against selected IV's - dirction = forward 
```{r echo=FALSE, warning=FALSE, comment=FALSE,cache=FALSE, results=FALSE}
lm_step<-step(lm(yy~.,data=na.omit(selectedOnes)),direction="forward")
```
###Final Model Summary 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
summary(lm_step)
```
####Same result - plots are same above



#Linear regression - NormB_user_score_healthy
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormB_user_score_healthy") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
```
##Simple Linear ression against each IV
```
ran simple linear regression against each IV 

selected any IV's - p-value is less than 0.10

```
### all available X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_food))
```
### not selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
x_vec<-selectingXs(xtable_food,y_vec)
print(colnames(xtable_food)[which(!(c(1:length(colnames(xtable_food))) %in% x_vec))])
```
### selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_food)[x_vec])
```

##Linear regression against selected IV's 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
selectedOnes<-createfinal(xtable_food,x_vec,y_vec)
slm<-lm(yy~.,data=selectedOnes)
summary(slm)
```
###Plots
```{r echo=FALSE, warning=FALSE}
drawhist(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawpp(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawscatter(slm,y_vec,y_name)
```

##Stepwise linear regression against selected IV's - dirction = forward 
```{r echo=FALSE, warning=FALSE, comment=FALSE,cache=FALSE, results=FALSE}
lm_step<-step(lm(yy~.,data=na.omit(selectedOnes)),direction="forward")
```
###Final Model Summary 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
summary(lm_step)
```
####Same result - plots are same above

#Linear regression - NormB_user_score_unhealthy
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormB_user_score_unhealthy") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
```
##Simple Linear ression against each IV
```
ran simple linear regression against each IV 

selected any IV's - p-value is less than 0.10

```
### all available X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_food))
```
### not selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
x_vec<-selectingXs(xtable_food,y_vec)
print(colnames(xtable_food)[which(!(c(1:length(colnames(xtable_food))) %in% x_vec))])
```
### selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_food)[x_vec])
```

##Linear regression against selected IV's 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
selectedOnes<-createfinal(xtable_food,x_vec,y_vec)
slm<-lm(yy~.,data=selectedOnes)
summary(slm)
```
###Plots
```{r echo=FALSE, warning=FALSE}
drawhist(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawpp(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawscatter(slm,y_vec,y_name)
```

##Stepwise linear regression against selected IV's - dirction = forward 
```{r echo=FALSE, warning=FALSE, comment=FALSE,cache=FALSE, results=FALSE}
lm_step<-step(lm(yy~.,data=na.omit(selectedOnes)),direction="forward")
```
###Final Model Summary 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
summary(lm_step)
```
####Same result - plots are same above

#Linear regression - NormB_user_num_alcohol_words
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormB_user_num_alcohol_words") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
```
##Simple Linear ression against each IV
```
ran simple linear regression against each IV 

selected any IV's - p-value is less than 0.10

```
### all available X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_alc))
```
### not selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
x_vec<-selectingXs(xtable_alc,y_vec)
print(colnames(xtable_alc)[which(!(c(1:length(colnames(xtable_alc))) %in% x_vec))])
```
### selected X's
```{r echo=FALSE, warning=FALSE}
options(width = 100)
print(colnames(xtable_alc)[x_vec])
```

##Linear regression against selected IV's 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
selectedOnes<-createfinal(xtable_alc,x_vec,y_vec)
slm<-lm(yy~.,data=selectedOnes)
summary(slm)
```
###Plots
```{r echo=FALSE, warning=FALSE}
drawhist(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawpp(slm,y_name)
```
```
```
```{r echo=FALSE, warning=FALSE}
drawscatter(slm,y_vec,y_name)
```

##Stepwise linear regression against selected IV's - dirction = forward 
```{r echo=FALSE, warning=FALSE, comment=FALSE,cache=FALSE, results=FALSE}
lm_step<-step(lm(yy~.,data=na.omit(selectedOnes)),direction="forward")
```
###Final Model Summary 
```{r echo=FALSE, warning=FALSE}
options(width = 300)
summary(lm_step)
```
####Same result - plots are same above



<!-- #Classification - predicting counties -->

<!-- ##Random Forests  -->

<!-- ###Creating a "true" test set -->
<!-- ``` -->
<!--                 80%           20% -->
<!-- County       class80       class_true       <-how I called -->
<!-- Data         data80        data_true        <-how I called -->

<!-- ``` -->
<!-- ```{r echo=FALSE, warning=FALSE} -->
<!-- set.seed(1) -->
<!-- # newxtable<-na.omit(xtable) -->
<!-- counties<-dt$countyname -->
<!-- countyNames<-unique(counties) -->
<!-- county_numeric<-seq(1,length(counties)) -->
<!-- thisCode <- 1  -->
<!-- for ( i in countyNames) { -->
<!--   if ( i== "."){ -->
<!--     county_numeric[which(counties == i)]<-NA -->
<!--   } -->
<!--   else{ -->
<!--     county_numeric[which(counties == i)]<-thisCode -->
<!--   thisCode <- thisCode+1  -->
<!--    } -->
<!-- } -->

<!-- class_dt<-cbind(county_numeric,xtable) -->
<!-- K <- 5 -->
<!-- n <- nrow(class_dt) -->
<!-- fold_assignments <- rep(1:K,length=n) -->
<!-- fold_assignments <- sample(fold_assignments) -->
<!-- myRandomNumber <- 5 -->
<!-- inds <- which(fold_assignments==myRandomNumber) -->

<!-- data_true <- as.matrix(class_dt[inds]) -->
<!-- data80 <- as.matrix(class_dt[-inds]) -->

<!-- dataColumnNames <- colnames(data80) -->
<!-- classColumn <- 1 -->
<!-- cols = c(1:ncol(data80)) -->
<!-- data_true <- sapply(cols, function(x) as.numeric(as.character(data_true[,x]))) -->
<!-- data80 <- sapply(cols, function(x) as.numeric(as.character(data80[,x]))) -->
<!-- ``` -->

<!-- ##1. Variable Screening -->
<!-- ```{r warning=FALSE} -->
<!-- dataCol <- function(ddff,cutoffs,classNum){ -->
<!--   cor_train<-cor(ddff) -->
<!--   inds<-which(abs(as.vector(cor_train[classNum,]))>cutoffs) -->
<!--   newdata80<-ddff[,inds] -->
<!--   return(newdata80) -->
<!-- } -->
<!-- dataCol_for_test <-function(test,ddff,cutoffs,classNum){ -->
<!--   cor_train<-cor(ddff) -->
<!--   inds<-which(abs(as.vector(cor_train[classNum,]))>cutoffs) -->
<!--   newtest<-test[,inds] -->
<!--   return(newtest) -->
<!-- } -->
<!-- nfoldsRf <- function(ddff,ccc,nfolds,seeed,classNum){ -->
<!--   #ddff, nfolds,seed -->
<!--   newddff<-dataCol(ddff,ccc,classNum) -->
<!--   set.seed(seeed) -->
<!--   K <- nfolds -->
<!--   n <- nrow(newddff) -->
<!--   fold_assignments <- rep(1:K,length=n) -->
<!--   fold_assignments <- sample(fold_assignments) -->
<!--   list_rf_county<-NULL -->

<!--   for( k in 1:K) { -->
<!--     test <- which(fold_assignments==k) -->
<!--     fold_train <- newddff[-test,] -->
<!--     fold_test <- newddff[test,] -->

<!--     rf_county<-randomForest(x=as.matrix(fold_train[,-classNum]),y=as.factor(fold_train[,classNum]),xtest=as.matrix(fold_test[,-classNum]),ytest=as.factor(fold_test[,classNum]),keep.forest=TRUE)   -->
<!--     list_rf_county[[k]]<-rf_county -->
<!--   } -->
<!--   return(list_rf_county) -->
<!-- } -->

<!-- # #--1--No removing  -->
<!-- # cutoff_list<-c(0,0.1,0.18,0.19,0.20,0.21,0.40) -->
<!-- #--2--removing log, Z, variables -->
<!-- cutoff_list<-c(0,0.1,0.14,0.15,0.16,0.17,0.18) -->
<!-- # #--3--removing original variables -->
<!-- # cutoff_list<-c(0,0.1,0.15,0.17,0.19,0.22,0.25) -->

<!-- rf1<-nfoldsRf(data80,cutoff_list[1],5,1,classColumn) -->
<!-- rf2<-nfoldsRf(data80,cutoff_list[2],5,1,classColumn) -->
<!-- rf3<-nfoldsRf(data80,cutoff_list[3],5,1,classColumn) -->
<!-- rf4<-nfoldsRf(data80,cutoff_list[4],5,1,classColumn) -->
<!-- rf5<-nfoldsRf(data80,cutoff_list[5],5,1,classColumn) -->
<!-- rf6<-nfoldsRf(data80,cutoff_list[6],5,1,classColumn) -->
<!-- rf7<-nfoldsRf(data80,cutoff_list[7],5,1,classColumn) -->
<!-- ``` -->
<!-- ```{r echo=FALSE,warning=FALSE} -->
<!-- big_list<-list(rf1,rf2,rf3,rf4,rf5,rf6,rf6,rf7) -->

<!-- cor_train<-cor(data80) -->
<!-- print("Number of variables      A cutoff point      Out-Of-Bag estimate of error rate") -->
<!-- store_cutoffs<-c() -->
<!-- store_oob<-c() -->

<!-- for (i in 1:7){ -->
<!--   rr<-big_list[[i]] -->
<!--   cc<-c() -->
<!--   for (j in 1:5){ -->
<!--     cc<-c(cc,colMeans(rr[[j]]$err.rate)[1]) -->
<!--   } -->
<!--   thisVariable<-length(which(abs(as.vector(cor_train[1,]))>cutoff_list[i])) -->
<!--   thisCutoff<-cutoff_list[i] -->
<!--   thisMean<-mean(cc) -->
<!--   print(paste(thisVariable,paste(thisCutoff,thisMean,sep="      "),sep="      ")) -->
<!--   store_cutoffs<-c(store_cutoffs,thisCutoff) -->
<!--   store_oob<-c(store_oob,thisMean) -->
<!-- } -->
<!-- selectedCutoff<-store_cutoffs[which.min(store_oob)] -->
<!-- ``` -->

<!-- ```{r echo=FALSE,warning=FALSE} -->
<!-- print(paste0("Cutoff point: ",paste0(selectedCutoff," has been chosen"))) -->
<!-- ``` -->

<!-- ##2. Runnning a model & 30 most import variables -->
<!-- ```{r warning=FALSE,fig.width=16,fig.height=10} -->
<!-- train <- dataCol(data80,selectedCutoff,classColumn) -->
<!-- test <- dataCol_for_test(data_true,data80,selectedCutoff,classColumn) -->
<!-- rf<-randomForest(x=train[,-1],y=as.factor(train[,1]),keep.forest=TRUE)  -->
<!-- pred_rf<-predict(rf,newdata=test[,-1],predict.all=T) -->
<!-- thisLabel<-rev(dataColumnNames[rev(order(rf$importance))][1:30]) -->
<!-- varImpPlot(rf,labels=thisLabel) -->
<!-- ``` -->

<!-- ##3. Final test -->
<!-- ``` -->
<!-- Genesee      Lapeer      Lenawee     Livingston     Macomb  -->
<!-- 26049        26087       26091       26093          26099 -->

<!-- Monroe       Oakland     St. Clair   Washtenaw      Wayne -->
<!-- 26115        26125       26147       26161          26163 -->
<!-- ``` -->
<!-- ```{r echo=FALSE, warning=FALSE} -->
<!-- confusionMatrix(pred_rf[[1]],test[,1]) -->
<!-- ``` -->


<!-- #Multinomial Logistic regression -Unhealthy, Neutral, Healthy --> -->
<!-- ##How did I create the values from NormA_tract_net_score? -->
<!-- ```{r echo=FALSE, warning=FALSE} -->
<!-- hist(dt$NormA_tract_net_score) -->
<!-- ``` -->

<!-- ``` -->
<!-- [min    to -0.2)    -->   -1 -->
<!-- [-0.2   to  0.2]    -->    0 -->
<!-- (0.2    to  max]    -->    1 -->
<!-- ``` -->

<!-- ```{r echo=FALSE, warning=FALSE} -->
<!-- dt[["logit"]]<-0 -->
<!-- for (i in 1:nrow(dt)){ -->
<!--   if (!(is.na(dt$NormA_tract_net_score[i]))) { -->
<!--     if(dt$NormA_tract_net_score[i]< (-0.1)){ -->
<!--     dt$logit[i]<- -1 -->
<!--   } else if (dt$NormA_tract_net_score[i]>(0.1)) { -->
<!--     dt$logit[i] <- 1 -->
<!--   } -->
<!--   } -->
<!-- } -->
<!-- dt$logit <- as.factor(dt$logit) -->
<!-- table(dt$logit) -->
<!-- ``` -->

