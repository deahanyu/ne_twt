---
title: "ne_twt_food"
author: "Deahan Yu"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown
---
```{r library, echo=FALSE, warning=FALSE, comment=FALSE, results=FALSE}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(data.table)
source("../../data/sources.r")
library(ggplot2)
library(gridExtra)
library(glmnet)
#library(knockoff)
#library(gam)
library(pls)
library(randomForest)
library(caret)
library(remef)
library(corrplot)
library(GGally)
library(faraway)
library(nortest)
```

```{r dataprep, echo=FALSE, warning=FALSE} 
## - - -  Uploading Data #fooddata == mydata2
dt <- data.table(read.csv("../../data/fooddata.csv",header = TRUE, sep = ','))
dt<-dt[,-(which(colnames(dt)=="num_healthy1"):which(colnames(dt)=="net_score"))]
dt<-dt[,-(which(colnames(dt)=="num_unique_users"):which(colnames(dt)=="net_healthy_unhealthy_related_tweets"))]

for (i in colnames(dt)) {
  if (any(i==c("GEOID10","tract","CountyCode","CountyName","countyname","highbroadbandadoptionrate"))){
    dt[,i] <- as.factor(as.character(dt[[i]]))
  } else if(length(grep("[.]1",i))>0 || length(grep("Mortality",i))>0 || length(grep("Sentiment",i))>0 || length(grep("total_twts",i))>0||length(grep("recrecen",i))>0||length(grep("Grocery_Store_AvgDistance",i))>0 || length(grep("Voter_Turnout",i))>0||length(grep("Parkland",i))>0||length(grep("Disability",i))>0) {
    #deleting variables that has .1 or .2 at the end.  (duplicated ones)
     dt[[i]]<-NULL 
  }
  else{
    dt[,i] <- as.numeric(as.character(dt[[i]]))
  }
}
dt<-dt[-1,]
#dropping variables that are >=400
maxNum <- ncol(dt)
for (j in maxNum:1){
  if (length(which(is.na(dt[[j]])==TRUE))>=1){
    if (length(which(is.na(dt[[j]])==TRUE))>=400){
      #print (paste(colnames(dt)[j],length(which(is.na(dt[[j]])==TRUE)),sep=" : "))
      dt[[j]] <- NULL
    }
  }
}
#IV's end after "GEOID"
temp<-dt$GEOID10
dt$GEOID10<-NULL
dt$GEOID10<-temp
dt$LOG10_PCT_21_to_29_09_13<-NULL
dt$LOG10_PCT_18_to_29_09_13<-NULL
dt$ZNeighborhood_Affluence_Index_2Vars_09_13<-NULL
dt$ZNeighborhood_Disadvantage_Index_3Vars_09_13<-NULL
IVnumbers<-which(colnames(dt)=="countyname")-1


```

#Normalization method 
##Two methods
```
Normalization A - Num food words
  Score healthy     /   Num food words
  Score unhealthy   /   Num food words
  Net score         /   Num food words

Normalization B - Food/Alcohol related tweets
  Score healthy     /   Food related tweets
  Score unhealthy   /   Food related tweets
  Net score         /   Food related tweets
  Num alcohol words /   All tweets
  
Tract-level 
  tweet-level  -->  tract-level  -->  normalization 

User-level
  tweet-level  -->  user-level   -->  normalization  --> user-count average of tract-level
```
##Plots
```
Left side compares normalzations                      Right side compares levels

Different norms   A vs. B                        Same norms         
Same level                                       Different level     Tact vs. User
```

```{r echo=FALSE,warning=FALSE}
prettyScatterplot(dt,dt$NormB_tract_score_healthy,"NormB_tract_score_healthy",dt$NormA_tract_score_healthy,"NormA_tract_score_healthy",dt$NormB_user_score_healthy,"NormB_user_score_healthy",dt$NormA_user_score_healthy,"NormA_user_score_healthy","score_healthy")

prettyScatterplot(dt,dt$NormB_tract_score_unhealthy,"NormB_tract_score_unhealthy",dt$NormA_tract_score_unhealthy,"NormA_tract_score_unhealthy",dt$NormB_user_score_unhealthy,"NormB_user_score_unhealthy",dt$NormA_user_score_unhealthy,"NormA_user_score_unhealthy","score_unhealthy")

prettyScatterplot(dt,dt$NormB_tract_net_score,"NormB_tract_net_score",dt$NormA_tract_net_score,"NormA_tract_net_score",dt$NormB_user_net_score,"NormB_user_net_score",dt$NormA_user_net_score,"NormA_user_net_score","net_score")

fMin<-min(summary(dt$NormB_tract_num_alcohol_words),summary(dt$NormB_user_num_alcohol_words))
fMax<-max(summary(dt$NormB_tract_num_alcohol_words),summary(dt$NormB_user_num_alcohol_words))
ggp<-ggplot(dt,aes(y=dt$NormB_tract_num_alcohol_words, x=dt$NormB_user_num_alcohol_words))+geom_point()+coord_cartesian(xlim=c(fMin,fMax),ylim=c(fMin,fMax))+geom_abline(slope=1, intercept=0)+labs(y="NormB_tract_num_alcohol_words",x="NormB_user_num_alcohol_words")
pseudodt<- data.frame()
ggp1<-ggplot(pseudodt) + geom_point() + xlim(0, 10) + ylim(0, 10)
grid.arrange(ggp1, ggp, ncol=2,top="num_alcohol_words")
```

#Variables
##Independent variables
```{r echo=FALSE, warning=FALSE}
#preserving only 80% of data with most twts
dt<-dt[rev(order(num_tweets))][1:round(nrow(dt)*0.8),]
# dt1<-dt[rev(order(num_tweets))][!(1:round(nrow(dt)*0.8)),]
# dt1$GEOID10

howmanyzz<-nrow(dt)
dt<-na.omit(dt)
thisValue <- 2 # this is for rstandard()
# indexLookup <- seq(1,nrow(dt))
# logVector<-c()

includeAll<-0#0 for including all points, 1 for removing outliers
#droping outliers
dropSome<-1




newcoldt<-c()
coldt<-colnames(dt)
for(i in coldt){
  if(length(grep("LOG10_.*",i)==1)){
    dt[[i]]<-scale(dt[[i]],center=TRUE,scale=TRUE)
    i <- paste0("Z",i)
  }
  else if(length(grep("Log_*",i)==1)){
    dt[[i]]<-scale(dt[[i]],center=TRUE,scale=TRUE)
    i <- paste0("Z",i)
  }
  else if(length(grep("PCT_*",i)==1)){
    dt[[i]]<-scale(dt[[i]],center=TRUE,scale=TRUE)
    i <- paste0("Z",i)
  }
  else if(length(grep("Pct_*",i)==1)){
    dt[[i]]<-scale(dt[[i]],center=TRUE,scale=TRUE)
    i <- paste0("Z",i)
  }
  else if(length(grep("X2015_*",i)==1)){
    dt[[i]]<-scale(dt[[i]],center=TRUE,scale=TRUE)
    i <- paste0("Z",i)
  }
  newcoldt <- c(newcoldt,i)
}

colnames(dt)<-newcoldt


#data prepartion for analyses
Z_LOG10_twts_14_16<-scale(log10(dt$num_tweets),center=TRUE,scale=TRUE)
xtable<-cbind(dt[,1:IVnumbers],Z_LOG10_twts_14_16)
xtable[["ZLOG10_twts_14_16"]]<-xtable[["V1"]]
xtable[["V1"]]<-NULL

xtable$ZX2015_disadvantage_SSI_pov_black<-NULL


#--For Food
#removing some variables
xtable_food<-xtable
xtable_food$ZPCT_age21_29<-NULL
xtable_food$ZLOG10_Liquor_Density_2015<-NULL
xtable_food$ZLOG10_Religious_Density_2015<-NULL
xtable_food$ZLog_Pct_HhIncome_Civilian_Pop_Over75K<-NULL
xtable_food$ZLog_Pct_Educ_25to64yrs_Higher_than_Associate_Est<-NULL
xtable_food$ZPct_LvgSSI_Est_Past12m<-NULL
xtable_food$ZPct_PovStatus_BelowPovLvl_Est.<-NULL
# xtable_food$ZLog_Pct_Race_Black_2001_Est<-NULL


#--For alcohol
#removing some variables
xtable_alc<-xtable
xtable_alc$ZPCT_age18_29<-NULL
xtable_alc$ZLOG10_fast_food_Density_2015<-NULL
xtable_alc$ZX2015_advantage_income_edu<-NULL
#xtable_alc$ZX2015_disadvantage_SSI_pov_black<-NULL
xtable_alc$ZX2015_disadvantage_SSI_pov<-NULL

```
###For food related y's
```{r echo=FALSE}
print(colnames(xtable_food))
```

###For alcohol related  y's
```{r echo=FALSE}
print(colnames(xtable_alc))
```

##Dependent variables
```{r echo=FALSE,warning=FALSE}
ys<-colnames(dt)[which(colnames(dt)=="NormA_tract_score_healthy"):which(colnames(dt)=="NormB_user_num_alcohol_words")]
ytable<-dt[,ys,with=FALSE]
print(ys)
```
```
  Normalization A : divided by Num food words
  Normalization B : divided by Food/Alcohol related tweets
```

#EDA of Variables
##Independent Variables
```{r echo=FALSE,warning=FALSE}
preEDAVar(xtable)
```

##Dependent Variabls
```{r echo=FALSE,warning=FALSE}
preEDAVar(ytable)

for (i in colnames(ytable)){
  if (length(grep("NormA_user",i))>0 || length(grep("NormB_user_num_alcohol",i))>0 ){
    print(i)
    print(ad.test(ytable[[i]]))
  }
}

```

#Pearson Correlation test on each Y's
```{r echo=FALSE,warning=FALSE}

options(width = 300)
xtable_food1<-xtable_food
colnames(xtable_food1)<-creatingNewColumnNames(xtable_food1,0)
xtable_alc1<-xtable_alc
colnames(xtable_alc1)<-creatingNewColumnNames(xtable_alc1,0)
for (i in 1:length(colnames(ytable))){
  if(colnames(ytable)[i]=="NormB_tract_num_alcohol_words"||colnames(ytable)[i]=="NormB_user_num_alcohol_words"){
    pairs(cbind(ytable[,i,with=FALSE],xtable_alc1),lower.panel=panel_cor)
  }else{
    pairs(cbind(ytable[,i,with=FALSE],xtable_food1),lower.panel=panel_cor)
  }
}

```

#Linear Regressions
```{r echo=FALSE,warning=FALSE}
cat(paste0("4. Only 80% of census tracts with most tweets were used \n",paste0("1593 tracts --> ",howmanyzz)))
```

```
For each regression,
          
          --> ran simple linear regression against each X 
          --> only X's with p-value less than 0.15 was selected 
          --> ran multivariate linear regression 

(Optional) --> dropped any influential points (out of the 95% of data). 
          
          --> re-ran the regression
          
          --> VIF test - collinearity test 



Table of contents (for below)

  Net score 
    NormA_user_net_score
    NormB_user_net_score
      
  Score Healthy
    NormA_user_score_healthy
    NormB_user_score_healthy
  
  Score Unhealthy      
  Alcohol (only for Norm B)
  
```

#net_score
##Normalization A - by the number of food words 
###NormA_user_net_score
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormA_user_net_score") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
```

###Keeping all points
```{r echo=FALSE,warning=FALSE,results='asis'}
multi_lm<-removingInflunentialPoints(xtable_food,y_vec,y_name,thisValue,includeAll)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```
####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```

###Dropping influential points
```{r echo=FALSE,warning=FALSE,results='asis'}
multi_lm<-removingInflunentialPoints(xtable_food,y_vec,y_name,thisValue,dropSome)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```

####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```

##Normalization B - by the number of tweets

###NormB_user_net_score
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormB_user_net_score") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
```

###Keeping all points
```{r echo=FALSE,warning=FALSE,results='asis'}
multi_lm<-removingInflunentialPoints(xtable_food,y_vec,y_name,thisValue,includeAll)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```
####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```

###Dropping influential points
```{r echo=FALSE,warning=FALSE,results='asis'}
multi_lm<-removingInflunentialPoints(xtable_food,y_vec,y_name,thisValue,dropSome)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```

####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```


#score_healthy
##Normalization A - by the number of food words 
###NormA_user_score_healthy
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormA_user_score_healthy") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
y_vec<-log10(y_vec)
```

###Keeping all points
```{r echo=FALSE,warning=FALSE,results='asis'}
multi_lm<-removingInflunentialPoints(xtable_food,y_vec,y_name,thisValue,includeAll)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```
####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```

###Dropping influential points
```{r echo=FALSE,warning=FALSE,results='asis'}

multi_lm<-removingInflunentialPoints(xtable_food,y_vec,y_name,thisValue,dropSome)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```

####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```

##Normalization B - by the number of tweets
###NormB_user_score_healthy
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormB_user_score_healthy") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
```

###Keeping all points
```{r echo=FALSE,warning=FALSE,results='asis'}
multi_lm<-removingInflunentialPoints(xtable_food,y_vec,y_name,thisValue,includeAll)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```
####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```

###Dropping influential points
```{r echo=FALSE,warning=FALSE,results='asis'}

multi_lm<-removingInflunentialPoints(xtable_food,y_vec,y_name,thisValue,dropSome)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```

####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```


#score_unhealthy
##Normalization A - by the number of food words 
###NormA_user_score_unhealthy
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormA_user_score_unhealthy") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
```

###Keeping all points
```{r echo=FALSE,warning=FALSE,results='asis'}
multi_lm<-removingInflunentialPoints(xtable_food,y_vec,y_name,thisValue,includeAll)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```
####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```

###Dropping influential points
```{r echo=FALSE,warning=FALSE,results='asis'}

multi_lm<-removingInflunentialPoints(xtable_food,y_vec,y_name,thisValue,dropSome)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```

####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```

##Normalization B - by the number of tweets
###NormB_user_score_unhealthy
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormB_user_score_unhealthy") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
print(y_name)
```

###Keeping all points
```{r echo=FALSE,warning=FALSE,results='asis'}
multi_lm<-removingInflunentialPoints(xtable_food,y_vec,y_name,thisValue,includeAll)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```
####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```

###Dropping influential points
```{r echo=FALSE,warning=FALSE,results='asis'}
multi_lm<-removingInflunentialPoints(xtable_food,y_vec,y_name,thisValue,dropSome)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```

####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```



#num_alcohol_words
###NormB_user_num_alcohol_words
```{r echo=FALSE, warning=FALSE}
thisY<-which(ys=="NormB_user_num_alcohol_words") #thisY follows the order of ys vector
y_name<-ys[thisY]
y_vec<-ytable[[y_name]]
y_vec<-log10(y_vec)
# xtable_alc_1 <- xtable_alc[-which(y_vec==-Inf)]
# y_vec_1 <- y_vec[-which(y_vec==-Inf)]
```

###Keeping all points
```{r echo=FALSE,warning=FALSE,results='asis'}
multi_lm<-removingInflunentialPoints(xtable_alc,y_vec,y_name,thisValue,includeAll)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```
####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```

###Dropping influential points
```{r echo=FALSE,warning=FALSE,results='asis'}
multi_lm<-removingInflunentialPoints(xtable_alc,y_vec,y_name,thisValue,dropSome)
```

```{r echo=FALSE}
options(width = 300)
summary(multi_lm)
```

####VIF - Collinearity test 
```{r echo=FALSE}
options(width = 20)
vif(multi_lm)
```

```{r echo=FALSE,warning=FALSE,results='asis'}
multiPlots(multi_lm,y_vec,y_name)
```


<!-- #num_alcohol_words - liquor only -->
<!-- ```{r echo=FALSE, warning=FALSE} -->
<!-- thisY<-which(ys=="NormB_user_num_alcohol_words") #thisY follows the order of ys vector -->
<!-- y_name<-ys[thisY] -->
<!-- y_vec<-ytable[[y_name]] -->
<!-- print(y_name) -->

<!-- ``` -->
<!-- ```{r } -->
<!-- plot(xtable_alc$ZLOG10_Liquor_Density_2015,y_vec, ylab=y_name,xlab="ZLOG10_Liquor_Density_2015") -->
<!-- summary(lm(y_vec~xtable_alc$ZLOG10_Liquor_Density_2015)) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- dt[which(y_vec>0.6),] -->
<!-- ``` -->

<!-- ```{r} -->
<!-- y_vec_new <- y_vec[-(which(y_vec>0.6))] -->
<!-- x_new <- xtable_alc$ZLOG10_Liquor_Density_2015[-(which(y_vec>0.6))] -->

<!-- plot(x_new,y_vec_new, ylab=y_name,xlab="ZLOG10_Liquor_Density_2015") -->
<!-- summary(lm(y_vec_new~x_new)) -->
<!-- ``` -->

<!-- ```{r} -->

<!-- dt_new<-dt[which(-1.7 < xtable_alc$ZLOG10_Liquor_Density_2015),c("ZLOG10_Liquor_Density_2015","NormB_user_num_alcohol_words")] -->


<!-- plot(dt_new$ZLOG10_Liquor_Density_2015,dt_new$NormB_user_num_alcohol_words) -->
<!-- summary(lm(NormB_user_num_alcohol_words~ZLOG10_Liquor_Density_2015,data=dt_new)) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- dt_new<-dt_new[-(which(NormB_user_num_alcohol_words>0.6)),] -->
<!-- plot(dt_new$ZLOG10_Liquor_Density_2015,dt_new$NormB_user_num_alcohol_words) -->
<!-- summary(lm(NormB_user_num_alcohol_words~ZLOG10_Liquor_Density_2015,data=dt_new)) -->

<!-- ``` -->
